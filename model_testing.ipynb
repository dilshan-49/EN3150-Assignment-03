{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85086d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5aacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform_standard = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=\"test_dataset\", transform=transform_standard)\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "class_names = test_dataset.classes\n",
    "    \n",
    "print(f\"Test dataset loaded successfully!\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d3aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolution blocks\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 16, kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # NN block\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(16*16*64, 512),     # 32768 -> 512\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 64),           # Another layer\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(64, 9)              # Output layer (9 waste categories)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        X = self.feature_extractor(x)\n",
    "        X = torch.flatten(X,1)\n",
    "        X = self.fully_connected(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872be049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model,ckpt_path, device):\n",
    "    \"\"\"\n",
    "    Load trained model from checkpoint\n",
    "    \n",
    "    Args:\n",
    "        model: Model architecture to load weights into\n",
    "        ckpt_path: Path to checkpoint file\n",
    "        device: Device to load model on (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded model in evaluation mode\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load checkpoint\n",
    "    print(f\"Loading checkpoint from: {ckpt_path}\")\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    \n",
    "    # Extract state dict (handles different checkpoint formats)\n",
    "    state_dict = ckpt.get(\"model_state_dict\", ckpt)\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(f\"Model loaded successfully on {device}\")\n",
    "    \n",
    "    # Print checkpoint info if available\n",
    "    if isinstance(ckpt, dict):\n",
    "        if 'epoch' in ckpt:\n",
    "            print(f\"Checkpoint epoch: {ckpt['epoch'] + 1}\")\n",
    "        if 'best_val_loss' in ckpt:\n",
    "            print(f\"Best validation loss: {ckpt['best_val_loss']:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516dd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_model(in_channel=3).to(device)\n",
    "\n",
    "model_adm = load_trained_model(model, \"adm_best_model.pth\", device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e8cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1) Recreate model architecture exactly as during training\n",
    "num_classes = len(class_names)  # set to your number of classes\n",
    "model = models.resnet50(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "model_resnet = load_trained_model(model, \"resnet_best_model.pth\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a22d228",
   "metadata": {},
   "source": [
    "## Evaluate Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, test_dataset, device):\n",
    "    \"\"\"\n",
    "    Evaluate model and return predictions and ground truth\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_loader: DataLoader for test dataset\n",
    "        test_dataset: Test dataset object\n",
    "        device: Device to run evaluation on\n",
    "    \n",
    "    Returns:\n",
    "        all_predictions: All predicted labels\n",
    "        all_labels: All ground truth labels\n",
    "        test_loss: Average test loss\n",
    "        test_accuracy: Test accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Calculate loss\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Get predictions\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_loss /= len(test_dataset)\n",
    "    test_accuracy = test_correct / len(test_dataset)\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels), test_loss, test_accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bebe1a",
   "metadata": {},
   "source": [
    "## Calculate and Display Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9649819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, labels, class_names):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, F1-score for each class\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted labels\n",
    "        labels: Ground truth labels\n",
    "        class_names: List of class names\n",
    "    \n",
    "    Returns:\n",
    "        metrics_dict: Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    # Calculate precision, recall, f1-score, support\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Calculate macro and weighted averages\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics_dict = {\n",
    "        'per_class': {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'support': support\n",
    "        },\n",
    "        'macro_avg': {\n",
    "            'precision': precision_macro,\n",
    "            'recall': recall_macro,\n",
    "            'f1_score': f1_macro\n",
    "        },\n",
    "        'weighted_avg': {\n",
    "            'precision': precision_weighted,\n",
    "            'recall': recall_weighted,\n",
    "            'f1_score': f1_weighted\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    print(classification_report(labels, predictions, target_names=class_names, zero_division=0))\n",
    "    \n",
    "    # Print per-class metrics in a formatted table\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PER-CLASS METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name:<15} {precision[i]:<12.4f} {recall[i]:<12.4f} \"\n",
    "              f\"{f1[i]:<12.4f} {support[i]:<10}\")\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Macro Avg':<15} {precision_macro:<12.4f} {recall_macro:<12.4f} \"\n",
    "          f\"{f1_macro:<12.4f} {len(labels):<10}\")\n",
    "    print(f\"{'Weighted Avg':<15} {precision_weighted:<12.4f} {recall_weighted:<12.4f} \"\n",
    "          f\"{f1_weighted:<12.4f} {len(labels):<10}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd4a90",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6be2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(predictions, labels, class_names, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted labels\n",
    "        labels: Ground truth labels\n",
    "        class_names: List of class names\n",
    "        save_path: Path to save the plot (optional)\n",
    "    \"\"\"\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot 1: Absolute counts\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'}, ax=axes[0])\n",
    "    axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('True Label', fontsize=12)\n",
    "    axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "    # Plot 2: Percentages\n",
    "    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Greens',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Percentage (%)'}, ax=axes[1])\n",
    "    axes[1].set_title('Confusion Matrix (Percentages)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('True Label', fontsize=12)\n",
    "    axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nConfusion matrix saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325ffee",
   "metadata": {},
   "source": [
    "## Main Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fa418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_evaluation(ckpt_path, test_data_path, device, batch_size=32, save_dir=\"evaluation_results\"):\n",
    "    \"\"\"\n",
    "    Complete evaluation pipeline\n",
    "    \n",
    "    Args:\n",
    "        ckpt_path: Path to model checkpoint\n",
    "        test_data_path: Path to test dataset folder\n",
    "        device: Device to run evaluation on\n",
    "        batch_size: Batch size for testing\n",
    "        save_dir: Directory to save evaluation results\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary containing all evaluation results\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"MODEL EVALUATION PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    \n",
    "    # 2. Load trained model\n",
    "    model = load_trained_model(ckpt_path, device)\n",
    "    \n",
    "    # 3. Evaluate model\n",
    "    predictions, labels, test_loss, test_accuracy = evaluate_model(\n",
    "        model, test_loader, test_dataset, device\n",
    "    )\n",
    "    \n",
    "    # 4. Print basic metrics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 5. Calculate detailed metrics\n",
    "    metrics_dict = calculate_metrics(predictions, labels, class_names)\n",
    "    \n",
    "    # 6. Plot confusion matrix\n",
    "    cm_save_path = os.path.join(save_dir, \"confusion_matrix.png\")\n",
    "    cm = plot_confusion_matrix(predictions, labels, class_names, save_path=cm_save_path)\n",
    "    \n",
    "    # 7. Compile results\n",
    "    results = {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'predictions': predictions,\n",
    "        'labels': labels,\n",
    "        'confusion_matrix': cm,\n",
    "        'metrics': metrics_dict,\n",
    "        'class_names': class_names\n",
    "    }\n",
    "    \n",
    "    # 8. Save results to file\n",
    "    results_file = os.path.join(save_dir, \"evaluation_results.txt\")\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(\"=\"*70 + \"\\n\")\n",
    "        f.write(\"MODEL EVALUATION RESULTS\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\\n\")\n",
    "        f.write(f\"Test Loss: {test_loss:.4f}\\n\")\n",
    "        f.write(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\\n\\n\")\n",
    "        f.write(\"Per-Class Metrics:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        f.write(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            p = metrics_dict['per_class']['precision'][i]\n",
    "            r = metrics_dict['per_class']['recall'][i]\n",
    "            f1 = metrics_dict['per_class']['f1_score'][i]\n",
    "            s = metrics_dict['per_class']['support'][i]\n",
    "            f.write(f\"{class_name:<15} {p:<12.4f} {r:<12.4f} {f1:<12.4f} {s:<10}\\n\")\n",
    "    \n",
    "    print(f\"\\nEvaluation results saved to: {results_file}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths\n",
    "ckpt_path = \"models/best_model_Adam.pth\"  # Change to your checkpoint path\n",
    "test_data_path = \"/kaggle/input/realwaste/realwaste-main/RealWaste\"  # Change to your test data path\n",
    "\n",
    "# Run full evaluation\n",
    "results = full_evaluation(\n",
    "    ckpt_path=ckpt_path,\n",
    "    test_data_path=test_data_path,\n",
    "    device=device,\n",
    "    batch_size=32,\n",
    "    save_dir=\"evaluation_results\"\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(f\"\\nFinal Test Accuracy: {results['test_accuracy']*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
